"""Package generation for multi-file output.

This module generates complete Python packages from CloudFormation templates,
organizing code into multiple files:

- __init__.py: Centralized imports and setup_resources()
- __main__.py: Entry point for python -m package_name
- main.py: Template builder function
- stack/__init__.py: Auto-discovery of resource files
- stack/main.py: Resources (or categorized files for large templates)

Key functions:
- generate_code(): Single-file output for simple templates
- generate_package(): Multi-file package output
"""

from __future__ import annotations

import re
from typing import TYPE_CHECKING

from cloudformation_dataclasses.constants import resolve_resource_type
from cloudformation_dataclasses.importer.ir import IRTemplate

from .classes import (
    generate_condition_class,
    generate_mapping_class,
    generate_output_class,
    generate_parameter_class,
    generate_resource_class,
)
from .context import (
    CodegenContext,
    PackageContext,
    build_arn_pattern_map,
    build_name_pattern_map,
)
from .helpers import get_resource_category, sanitize_class_name
from .imports import generate_imports
from .topology import (
    find_resource_dependencies,
    find_strongly_connected_components,
    order_scc_resources,
    topological_sort,
)
from .values import escape_string

if TYPE_CHECKING:
    pass


# =============================================================================
# Single File Generation
# =============================================================================


def generate_code(
    template: IRTemplate,
    include_main: bool = True,
) -> str:
    """
    Generate Python code from analyzed IR.

    Args:
        template: Parsed IRTemplate
        include_main: Include if __name__ == "__main__" block

    Returns:
        Complete Python module source code
    """
    ctx = CodegenContext(
        template=template,
        include_main_block=include_main,
    )

    # Build pattern maps for implicit ref and get_att detection
    ctx.name_pattern_map = build_name_pattern_map(template)
    ctx.arn_pattern_map = build_arn_pattern_map(template)

    # Add ref and get_att imports (commonly needed)
    ctx.add_import("cloudformation_dataclasses.core", "ref")
    ctx.add_import("cloudformation_dataclasses.core", "get_att")

    sections: list[str] = []

    # Module docstring
    if template.description:
        sections.append(f'"""\n{template.description}\n\nGenerated by cfn-dataclasses-import.\n"""')
    else:
        sections.append('"""Generated by cfn-dataclasses-import."""')

    code = _generate_block_mode(template, ctx, sections, include_main)

    return code


def _generate_block_mode(
    template: IRTemplate,
    ctx: CodegenContext,
    sections: list[str],
    include_main: bool,
) -> str:
    """Generate block mode (declarative) code."""
    class_sections: list[str] = []

    # Parameters
    for param in template.parameters.values():
        class_sections.append(generate_parameter_class(param, ctx))

    # Mappings
    for mapping in template.mappings.values():
        class_sections.append(generate_mapping_class(mapping, ctx))

    # Conditions
    for condition in template.conditions.values():
        class_sections.append(generate_condition_class(condition, ctx))

    # Resources (in dependency order)
    # For each resource, first insert any PropertyType wrappers generated for it,
    # then the resource class itself. This ensures proper ordering since
    # PropertyType wrappers may use ref() to reference other resources.
    sorted_resources = topological_sort(template)
    for resource_id in sorted_resources:
        resource = template.resources[resource_id]
        # Clear PropertyType class defs before generating this resource
        ctx.property_type_class_defs = []
        resource_class = generate_resource_class(resource, ctx)
        # Add PropertyType wrappers for this resource (generated during resource class generation)
        class_sections.extend(ctx.property_type_class_defs)
        # Then add the resource class itself
        class_sections.append(resource_class)

    # Outputs
    for output in template.outputs.values():
        class_sections.append(generate_output_class(output, ctx))

    # No template wrapper class - resources auto-register via @cloudformation_dataclass
    # Use Template.from_registry() in build_template()

    # Now generate imports
    ctx.add_import("cloudformation_dataclasses.core", "Template")
    sections.append(generate_imports(ctx))

    # Add class sections (filter out empty strings)
    sections.extend(s for s in class_sections if s)

    # Build template function using Template.from_registry()
    # Build parameter list if any
    param_list = ""
    if template.parameters:
        param_names = ", ".join(template.parameters.keys())
        param_list = f"parameters=[{param_names}]"

    # Build output list if any
    output_list = ""
    if template.outputs:
        output_names = ", ".join(f"{lid}Output" for lid in template.outputs.keys())
        output_list = f"outputs=[{output_names}]"

    # Build the from_registry call
    args = []
    if template.description:
        args.append(f"description={escape_string(template.description)}")
    if param_list:
        args.append(param_list)
    if output_list:
        args.append(output_list)

    if args:
        args_str = ",\n        ".join(args)
        from_registry_call = f"Template.from_registry(\n        {args_str},\n    )"
    else:
        from_registry_call = "Template.from_registry()"

    sections.append(
        f"""
def build_template() -> Template:
    \"\"\"Build the CloudFormation template.\"\"\"
    return {from_registry_call}"""
    )

    # Main block
    if include_main:
        sections.append(
            """
if __name__ == "__main__":
    import json
    template = build_template()
    print(json.dumps(template.to_dict(), indent=2))"""
        )

    return "\n\n\n".join(sections) + "\n"


# =============================================================================
# Package Generation
# =============================================================================


def generate_package(template: IRTemplate, package_name: str) -> dict[str, str]:
    """
    Generate a Python package from template (multi-file output).

    Args:
        template: Parsed IRTemplate
        package_name: Name of the Python package (used as subdirectory prefix)

    Returns:
        Dict mapping filename to content, with all files prefixed by package_name/:
        - "{package_name}/__init__.py": Centralized imports (re-exports from .stack)
        - "{package_name}/main.py": build_template + entry point
        - "{package_name}/__main__.py": Entry point for python -m
        - "{package_name}/stack/": Directory with params, outputs, and resources
    """
    pkg_ctx = PackageContext(template=template)

    # Initialize codegen context
    ctx = pkg_ctx.codegen_ctx
    ctx.add_import("cloudformation_dataclasses.core", "ref")
    ctx.add_import("cloudformation_dataclasses.core", "get_att")
    ctx.add_import("cloudformation_dataclasses.core", "Template")

    # Pre-pass: detect AWS class name collisions across modules
    # We need to know this before generating resource files so they can use qualified names
    _detect_aws_name_collisions(ctx, template)

    # Generate stack/params.py (Parameters, Mappings, Conditions)
    params_content = _generate_params_py(pkg_ctx, template)

    # Generate stack/outputs.py (if there are outputs)
    outputs_content = _generate_outputs_py(pkg_ctx, template)

    # Generate stack/ package (consolidated resources + __init__.py)
    stack_files = _generate_stack_package(pkg_ctx, template)

    # Generate main.py (build_template)
    main_content = _generate_main_py(pkg_ctx, template)

    # Generate __init__.py last (after all imports collected)
    init_content = _generate_init_py(pkg_ctx, template)

    # Generate __main__.py for `python -m package_name` support
    dunder_main_content = f'''"""Allow running as: python -m {package_name}."""
from .main import main

main()
'''

    # Prefix all files with package_name/ for nested package structure
    files = {
        f"{package_name}/__init__.py": init_content,
        f"{package_name}/__main__.py": dunder_main_content,
        f"{package_name}/main.py": main_content,
        f"{package_name}/stack/params.py": params_content,
    }

    # Add stack/outputs.py if there are outputs
    if outputs_content:
        files[f"{package_name}/stack/outputs.py"] = outputs_content

    # Add all stack files (already have stack/ prefix, add package_name/)
    for filename, content in stack_files.items():
        files[f"{package_name}/{filename}"] = content

    return files


# =============================================================================
# Init File Generation
# =============================================================================


def _generate_init_py(pkg_ctx: PackageContext, template: IRTemplate) -> str:
    """Generate __init__.py with centralized imports."""
    lines = []

    # Docstring from template description
    if template.description:
        # Truncate long descriptions
        desc = template.description.strip()
        if len(desc) > 200:
            desc = desc[:197] + "..."
        lines.append(f'"""{desc}\n\nGenerated by cfn-dataclasses-import."""')
    else:
        lines.append('"""Generated by cfn-dataclasses-import."""')

    lines.append("")

    # Generate import statements from collected imports
    ctx = pkg_ctx.codegen_ctx

    # Core imports
    core_imports = ctx.imports.get("cloudformation_dataclasses.core", set())
    # Always include these
    core_imports.add("cloudformation_dataclass")
    core_imports.add("ref")
    core_imports.add("get_att")
    core_imports.add("Template")

    if core_imports:
        sorted_imports = sorted(core_imports)
        if len(sorted_imports) <= 3:
            lines.append(
                f"from cloudformation_dataclasses.core import {', '.join(sorted_imports)}"
            )
        else:
            lines.append("from cloudformation_dataclasses.core import (")
            for name in sorted_imports:
                lines.append(f"    {name},")
            lines.append(")")

    # AWS module imports (e.g., from cloudformation_dataclasses.aws import s3)
    # This is needed when wrapper class names collide with AWS resource class names
    aws_base = ctx.imports.get("cloudformation_dataclasses.aws", set())

    # Add common AWS imports for init/skeleton mode
    if not template.resources and not aws_base:
        lines.append("from cloudformation_dataclasses.aws import ec2, iam, lambda_, s3")
        aws_base = {"ec2", "iam", "lambda_", "s3"}  # Track for __all__ generation
    elif aws_base:
        sorted_modules = sorted(aws_base)
        if len(sorted_modules) <= 3:
            lines.append(
                f"from cloudformation_dataclasses.aws import {', '.join(sorted_modules)}"
            )
        else:
            lines.append("from cloudformation_dataclasses.aws import (")
            for name in sorted_modules:
                lines.append(f"    {name},")
            lines.append(")")

    # AWS submodule imports (e.g., from cloudformation_dataclasses.aws.s3 import Bucket)
    aws_modules = sorted(
        (mod, names)
        for mod, names in ctx.imports.items()
        if mod.startswith("cloudformation_dataclasses.aws.")
    )

    # Detect name collisions across AWS modules
    # When multiple modules export the same name (e.g., Policy from iam and iot),
    # we need to use qualified imports instead of direct imports
    all_aws_names: dict[str, list[str]] = {}
    for module, names in aws_modules:
        for name in names:
            all_aws_names.setdefault(name, []).append(module)

    colliding_names = {name for name, mods in all_aws_names.items() if len(mods) > 1}

    # Store colliding names in context so resource file codegen can use qualified access
    # Map each colliding name to its module (for cases where we know which module to use)
    for name in colliding_names:
        for mod in all_aws_names[name]:
            short_mod = mod.split(".")[-1]  # e.g., 'iot' from '...aws.iot'
            # Store name -> module mapping for use in resource codegen
            # If multiple modules have the same name, store all of them
            ctx.colliding_aws_names[name] = short_mod  # Last one wins, but we also track in aws_base

    # For colliding names, add their modules to aws_base for qualified access
    if colliding_names:
        if aws_base is None:
            aws_base = set()
        for name in colliding_names:
            for mod in all_aws_names[name]:
                short_mod = mod.split(".")[-1]  # e.g., 'iot' from '...aws.iot'
                aws_base.add(short_mod)

        # Re-emit aws_base imports if we added new modules
        # (only if we didn't already emit them above)
        if not ctx.imports.get("cloudformation_dataclasses.aws"):
            sorted_modules = sorted(aws_base)
            if len(sorted_modules) <= 3:
                lines.append(
                    f"from cloudformation_dataclasses.aws import {', '.join(sorted_modules)}"
                )
            else:
                lines.append("from cloudformation_dataclasses.aws import (")
                for name in sorted_modules:
                    lines.append(f"    {name},")
                lines.append(")")

    for module, names in aws_modules:
        # Only import non-colliding names directly
        direct_names = sorted(n for n in names if n not in colliding_names)
        if direct_names:
            if len(direct_names) <= 3:
                lines.append(f"from {module} import {', '.join(direct_names)}")
            else:
                lines.append(f"from {module} import (")
                for name in direct_names:
                    lines.append(f"    {name},")
                lines.append(")")

    # Intrinsic imports
    if ctx.intrinsic_imports:
        sorted_intrinsics = sorted(ctx.intrinsic_imports)
        if len(sorted_intrinsics) <= 3:
            lines.append(
                f"from cloudformation_dataclasses.intrinsics import "
                f"{', '.join(sorted_intrinsics)}"
            )
        else:
            lines.append("from cloudformation_dataclasses.intrinsics import (")
            for name in sorted_intrinsics:
                lines.append(f"    {name},")
            lines.append(")")

    lines.append("")

    # Re-export everything from .stack (params, outputs, resources)
    # This provides `from packagename import ResourceName` access
    lines.append("from .stack import *  # noqa: F403, F401")
    lines.append("")

    # Collect names for __all__
    config_names: list[str] = []
    for param in template.parameters.values():
        config_names.append(sanitize_class_name(param.logical_id))
    for mapping in template.mappings.values():
        config_names.append(f"{sanitize_class_name(mapping.logical_id)}Mapping")
    for condition in template.conditions.values():
        config_names.append(f"{sanitize_class_name(condition.logical_id)}Condition")

    resource_names: list[str] = []
    for resource in template.resources.values():
        resource_names.append(sanitize_class_name(resource.logical_id))

    output_names: list[str] = []
    for output in template.outputs.values():
        output_names.append(f"{sanitize_class_name(output.logical_id)}Output")

    # Generate __all__ with all exported names
    # Exclude colliding names - they're accessed via module (e.g., iot.Policy)
    direct_aws_names = set().union(*[names for _, names in aws_modules]) - colliding_names
    all_names = sorted(core_imports | direct_aws_names)
    # Add AWS module names (e.g., 's3') for qualified access like s3.Bucket
    if aws_base:
        all_names = sorted(set(all_names) | aws_base)
    if ctx.intrinsic_imports:
        all_names = sorted(set(all_names) | ctx.intrinsic_imports)
    # Add config names to __all__
    all_names = sorted(set(all_names) | set(config_names))
    # Add resource names to __all__
    all_names = sorted(set(all_names) | set(resource_names))
    # Add output names to __all__
    all_names = sorted(set(all_names) | set(output_names))

    lines.append("__all__ = [")
    for name in all_names:
        lines.append(f'    "{name}",')
    lines.append("]")

    return "\n".join(lines) + "\n"


# =============================================================================
# Params File Generation
# =============================================================================


def _generate_params_py(pkg_ctx: PackageContext, template: IRTemplate) -> str:
    """Generate stack/params.py with Parameters, Mappings, Conditions."""
    lines = []
    lines.append('"""Parameters, Mappings, and Conditions."""')
    lines.append("")
    lines.append("from .. import *  # noqa: F403")
    lines.append("")
    lines.append("")

    ctx = pkg_ctx.codegen_ctx

    # Parameters
    for param in template.parameters.values():
        class_def = generate_parameter_class(param, ctx)
        lines.append(class_def)
        lines.append("")
        lines.append("")
        pkg_ctx.config_exports.add(sanitize_class_name(param.logical_id))

    # Mappings
    for mapping in template.mappings.values():
        class_def = generate_mapping_class(mapping, ctx)
        lines.append(class_def)
        lines.append("")
        lines.append("")
        pkg_ctx.config_exports.add(f"{sanitize_class_name(mapping.logical_id)}Mapping")

    # Conditions
    for condition in template.conditions.values():
        class_def = generate_condition_class(condition, ctx)
        lines.append(class_def)
        lines.append("")
        lines.append("")
        pkg_ctx.config_exports.add(f"{sanitize_class_name(condition.logical_id)}Condition")

    # Remove trailing empty lines
    while lines and lines[-1] == "":
        lines.pop()

    return "\n".join(lines) + "\n"


# =============================================================================
# Outputs File Generation
# =============================================================================


def _generate_outputs_py(pkg_ctx: PackageContext, template: IRTemplate) -> str | None:
    """Generate stack/outputs.py with Output definitions.

    Returns None if there are no outputs to generate.
    """
    if not template.outputs:
        return None

    lines = []
    lines.append('"""Template outputs."""')
    lines.append("")
    lines.append("from .. import *  # noqa: F403")
    lines.append("")
    lines.append("")

    ctx = pkg_ctx.codegen_ctx

    # Outputs
    for output in template.outputs.values():
        class_def = generate_output_class(output, ctx)
        lines.append(class_def)
        lines.append("")
        lines.append("")
        pkg_ctx.outputs_exports.add(f"{output.logical_id}Output")

    # Remove trailing empty lines
    while lines and lines[-1] == "":
        lines.pop()

    return "\n".join(lines) + "\n"


# =============================================================================
# Main File Generation
# =============================================================================


def _generate_main_py(pkg_ctx: PackageContext, template: IRTemplate) -> str:
    """Generate main.py with build_template and __main__."""
    lines = []
    lines.append('"""Template builder."""')
    lines.append("")
    # Import from parent package (gets Template + all stack re-exports)
    lines.append("from . import *  # noqa: F403, F401")
    lines.append("")
    lines.append("")

    # Build template function
    param_list = ""
    if template.parameters:
        param_names = ", ".join(template.parameters.keys())
        param_list = f"parameters=[{param_names}]"

    output_list = ""
    if template.outputs:
        output_names = ", ".join(f"{lid}Output" for lid in template.outputs.keys())
        output_list = f"outputs=[{output_names}]"

    args = []
    if template.description:
        args.append(f"description={escape_string(template.description)}")
    if param_list:
        args.append(param_list)
    if output_list:
        args.append(output_list)

    if args:
        args_str = ",\n        ".join(args)
        from_registry_call = f"Template.from_registry(\n        {args_str},\n    )"
    else:
        from_registry_call = "Template.from_registry()"

    lines.append("def build_template() -> Template:")
    lines.append('    """Build the CloudFormation template."""')
    lines.append(f"    return {from_registry_call}")
    lines.append("")
    lines.append("")
    lines.append("def main() -> None:")
    lines.append('    """Print the CloudFormation template as JSON."""')
    lines.append("    import json")
    lines.append("    template = build_template()")
    lines.append("    print(json.dumps(template.to_dict(), indent=2))")
    lines.append("")
    lines.append("")
    lines.append('if __name__ == "__main__":')
    lines.append("    main()")

    return "\n".join(lines) + "\n"


# =============================================================================
# Stack Package Generation
# =============================================================================


def _generate_stack_package(pkg_ctx: PackageContext, template: IRTemplate) -> dict[str, str]:
    """Generate stack/ directory with params, outputs, and consolidated resources.

    Resources are consolidated into fewer files:
    - If total resources < 5: all go in main.py
    - SCCs (cyclic dependencies) always go in main.py
    - Single-resource groups also go in main.py
    - Only large templates with multiple non-SCC resources get separate files

    Returns:
        Dict mapping "stack/<filename>.py" to content
    """
    files = {}
    ctx = pkg_ctx.codegen_ctx

    # Find strongly connected components to detect cycles
    sccs = find_strongly_connected_components(template)

    # Map each resource to its SCC (for grouping)
    resource_to_scc: dict[str, int] = {}
    for scc_idx, scc in enumerate(sccs):
        for resource_id in scc:
            resource_to_scc[resource_id] = scc_idx

    # Build SCC ordering info for forward reference detection
    scc_orderings: dict[int, list[str]] = {}  # scc_idx -> ordered list of resource IDs
    for scc_idx, scc in enumerate(sccs):
        if len(scc) > 1:
            scc_orderings[scc_idx] = order_scc_resources(scc, template)

    # First pass: generate all resources and collect their PropertyType classes
    # For SCC members, set up forward_references before generating
    sorted_resources = topological_sort(template)
    resource_classes: dict[str, tuple[str, list[str]]] = {}  # id -> (class_def, [pt_defs])

    for resource_id in sorted_resources:
        resource = template.resources[resource_id]
        ctx.property_type_class_defs = []

        # Set up forward references for SCC members
        scc_idx = resource_to_scc[resource_id]
        if scc_idx in scc_orderings:
            ordered_scc = scc_orderings[scc_idx]
            # Resources after this one in the ordering are forward references
            resource_pos = ordered_scc.index(resource_id)
            ctx.forward_references = set(ordered_scc[resource_pos + 1:])
        else:
            ctx.forward_references = set()

        resource_class = generate_resource_class(resource, ctx)
        resource_classes[resource_id] = (resource_class, list(ctx.property_type_class_defs))
        pkg_ctx.resources_exports.add(resource_id)

    # Clear forward_references after generation
    ctx.forward_references = set()

    # Second pass: decide which resources go to main.py vs separate files
    # Collect resources for main.py and track separate files
    main_py_resources: list[str] = []  # ordered list of resource IDs for main.py
    separate_files: dict[str, list[str]] = {}  # filename -> ordered resource IDs
    generated_sccs: set[int] = set()

    for resource_id in sorted_resources:
        scc_idx = resource_to_scc[resource_id]
        if scc_idx in generated_sccs:
            continue
        generated_sccs.add(scc_idx)

        scc = sccs[scc_idx]

        if len(scc) > 1:
            # SCC (cyclic dependencies) goes to main.py
            main_py_resources.extend(scc_orderings[scc_idx])
        else:
            # Single resource: group by category
            resource = template.resources[resource_id]
            category = get_resource_category(resource)
            if category == "main":
                main_py_resources.append(resource_id)
            else:
                if category not in separate_files:
                    separate_files[category] = []
                separate_files[category].append(resource_id)

    # Build file-level dependency graph and move resources to break import cycles
    # This is necessary because category files are loaded dynamically by setup_resources()
    # which uses topological sort - if there's a cycle, it can't resolve correctly.
    #
    # Example cycle: compute.py (EC2) -> network.py (SecurityGroup) -> main.py (CustomResource) -> compute.py
    # Fix: move resources from category files to main.py until no cycles remain
    resource_to_file: dict[str, str] = {}
    for rid in main_py_resources:
        resource_to_file[rid] = "main"
    for cat, rids in separate_files.items():
        for rid in rids:
            resource_to_file[rid] = cat

    # Build resource dependency map (resource_id -> set of resource_ids it depends on)
    # Use only explicit references (reference_graph + DependsOn), not Sub pattern matching
    all_resource_deps: dict[str, set[str]] = {}
    for rid in template.resources:
        all_resource_deps[rid] = find_resource_dependencies(template, rid)

    # Iteratively move resources to main.py to break file-level cycles
    max_iterations = len(template.resources) + 1
    for _ in range(max_iterations):
        # Build file-level dependency graph
        file_deps: dict[str, set[str]] = {"main": set()}
        for cat in separate_files:
            file_deps[cat] = set()

        for rid, rid_deps in all_resource_deps.items():
            rid_file = resource_to_file.get(rid, "main")
            for dep_rid in rid_deps:
                dep_file = resource_to_file.get(dep_rid, "main")
                if dep_file != rid_file:
                    file_deps[rid_file].add(dep_file)

        # Detect cycles using DFS
        def find_cycle() -> list[str] | None:
            visited: set[str] = set()
            rec_stack: set[str] = set()
            path: list[str] = []

            def dfs(node: str) -> list[str] | None:
                visited.add(node)
                rec_stack.add(node)
                path.append(node)

                for neighbor in file_deps.get(node, set()):
                    if neighbor not in visited:
                        result = dfs(neighbor)
                        if result:
                            return result
                    elif neighbor in rec_stack:
                        # Found cycle - return the cycle path
                        cycle_start = path.index(neighbor)
                        return path[cycle_start:]

                path.pop()
                rec_stack.remove(node)
                return None

            for node in file_deps:
                if node not in visited:
                    result = dfs(node)
                    if result:
                        return result
            return None

        cycle = find_cycle()
        if not cycle:
            break  # No cycles, we're done

        # Move all resources from non-main cycle members to main.py
        for cycle_file in cycle:
            if cycle_file != "main" and cycle_file in separate_files:
                # Move all resources from this category to main
                for rid in separate_files[cycle_file]:
                    main_py_resources.append(rid)
                    resource_to_file[rid] = "main"
                del separate_files[cycle_file]

    # Re-sort resources in topological order to ensure dependencies come first
    topo_order = {rid: idx for idx, rid in enumerate(sorted_resources)}
    main_py_resources.sort(key=lambda rid: topo_order[rid])
    for category in separate_files:
        separate_files[category].sort(key=lambda rid: topo_order[rid])

    # Generate main.py with all consolidated resources
    if main_py_resources:
        lines = []
        lines.append('"""Stack resources."""')
        lines.append("")
        lines.append("from .. import *  # noqa: F403")
        lines.append("")
        lines.append("")

        for res_id in main_py_resources:
            resource_class, pt_defs = resource_classes[res_id]

            # PropertyType wrappers for this resource
            for pt_def in pt_defs:
                lines.append(pt_def)
                lines.append("")
                lines.append("")

            # The resource class itself
            lines.append(resource_class)
            lines.append("")
            lines.append("")

        # Remove trailing empty lines
        while lines and lines[-1] == "":
            lines.pop()

        files["stack/main.py"] = "\n".join(lines) + "\n"

    # Generate category files (compute.py, network.py, etc.)
    for filename, resource_ids in separate_files.items():
        lines = []
        resource_names = ", ".join(resource_ids)
        lines.append(f'"""{filename.title()} resources: {resource_names}."""')
        lines.append("")
        lines.append("from .. import *  # noqa: F403")
        lines.append("")
        lines.append("")

        for res_id in resource_ids:
            resource_class, pt_defs = resource_classes[res_id]

            for pt_def in pt_defs:
                lines.append(pt_def)
                lines.append("")
                lines.append("")

            lines.append(resource_class)
            lines.append("")
            lines.append("")

        while lines and lines[-1] == "":
            lines.pop()

        files[f"stack/{filename}.py"] = "\n".join(lines) + "\n"

    # Generate stack/__init__.py that re-exports params, outputs, and resources
    init_lines = ['"""Stack - parameters, outputs, and resources."""']
    init_lines.append("from .params import *  # noqa: F403, F401")
    init_lines.append("")
    init_lines.append("# Import resources with topological ordering")
    init_lines.append("from cloudformation_dataclasses.core.resource_loader import setup_resources")
    init_lines.append('setup_resources(__file__, __name__, globals())')
    init_lines.append("")
    init_lines.append("# Import outputs after resources (outputs reference resource classes)")
    init_lines.append("try:")
    init_lines.append("    from .outputs import *  # noqa: F403, F401")
    init_lines.append("except ImportError:")
    init_lines.append("    pass")
    init_lines.append("")
    files["stack/__init__.py"] = "\n".join(init_lines)

    return files


# =============================================================================
# Collision Detection
# =============================================================================


def _detect_aws_name_collisions(ctx: CodegenContext, template: IRTemplate) -> None:
    """
    Pre-scan template to detect AWS class names that appear in multiple modules.

    When multiple resources use the same class name from different modules
    (e.g., iam.Policy and iot.Policy), we need to use qualified imports
    to avoid Python import collisions.

    Populates ctx.colliding_aws_names with name -> module mapping.
    """
    # Track all AWS class names and their modules
    aws_class_to_modules: dict[str, set[str]] = {}

    for resource in template.resources.values():
        resolved = resolve_resource_type(resource.resource_type)
        if resolved:
            module, class_name = resolved
            aws_class_to_modules.setdefault(class_name, set()).add(module)

    # Find colliding names (same class name in multiple modules)
    for class_name, modules in aws_class_to_modules.items():
        if len(modules) > 1:
            # This name appears in multiple modules - mark as colliding
            # Store each module for the colliding name
            for module in modules:
                ctx.colliding_aws_names[class_name] = module  # Last one stored


# =============================================================================
# Reference Finding Utilities
# =============================================================================


def _find_config_references(template: IRTemplate) -> set[str]:
    """Find parameter/mapping/condition names referenced by resources."""
    refs = set()
    for resource in template.resources.values():
        # Check for references in the reference graph
        for ref_target in template.reference_graph.get(resource.logical_id, []):
            if ref_target in template.parameters:
                refs.add(ref_target)
            elif ref_target in template.mappings:
                refs.add(f"{ref_target}Mapping")
            elif ref_target in template.conditions:
                refs.add(f"{ref_target}Condition")
    return refs


def _find_resource_references_in_outputs(template: IRTemplate) -> set[str]:
    """Find resource names referenced by outputs (for get_att/ref)."""
    refs = set()
    for output in template.outputs.values():
        # Check reference graph for this output
        for ref_target in template.reference_graph.get(output.logical_id, []):
            if ref_target in template.resources:
                refs.add(ref_target)
    return refs
